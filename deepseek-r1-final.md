### DeepSeek-R1 技术资料整理（校正版）

#### **一、架构设计**

1. **基础架构**  
   - **基座模型**：基于DeepSeek-V3-Base（非MoE架构），未采用MLA技术。  
   - **模型规模**：总参数量671B，激活参数37B（见表4）。  
   - **核心优化**：通过强化学习（RL）提升推理能力，而非依赖模型架构扩展。

---

#### **二、训练技术**
1. **强化学习框架**  
   - **GRPO算法**：采用Group Relative Policy Optimization（Shao et al., 2024），通过组内评分替代传统Critic模型，降低训练成本。  
     **核心公式修正**：  
     $$
     \mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}\left[ \frac{1}{G} \sum_{i=1}^{G} \left( \min\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathbb{D}_{\text{KL}} \left( \pi_{\theta} \| \pi_{\text{ref}} \right) \right) \right]
     $$
     
     
     
     其中：  
     
     - **优势值 \(A_i\)**：由组内奖励标准化计算（公式3）：  
       
     - $$
       A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}
       $$
       
       
       
     - **KL散度项**：  
       $$
       \mathbb{D}_{\text{KL}} \left( \pi_{\theta} \| \pi_{\text{ref}} \right) = \frac{\pi_{\text{ref}}(o_i|q)}{\pi_{\theta}(o_i|q)} - \log \frac{\pi_{\text{ref}}(o_i|q)}{\pi_{\theta}(o_i|q)} - 1
       $$
       
     
   - **奖励设计**：  
     - **准确性奖励**：基于规则验证最终答案（如数学问题结果匹配、代码编译通过）。  
     - **格式奖励**：强制模型使用 `<think>` 和 `<answer>` 标签分隔推理过程与答案（见表1）。
   
2. **多阶段训练流程**  
   - **DeepSeek-R1-Zero（纯RL）**：直接在基础模型上应用RL，无监督微调（SFT），自主演化出反思、长链推理等能力。  
   - **DeepSeek-R1（冷启动+RL）**：  
     - **冷启动数据**：收集数千条长链推理数据，格式为 `|special_token|<reasoning_process>|special_token|<summary>`，通过SFT提升初始可读性。  
     - **两阶段RL**：  
       1. **首阶段**：专注推理任务，引入语言一致性奖励（目标语言词汇占比）以优化输出可读性。  
       2. **次阶段**：结合通用任务对齐人类偏好（如写作、事实问答）。  
     - **拒绝采样与SFT**：RL收敛后，通过拒绝采样生成600K推理数据（过滤混合语言/冗长输出）和200K非推理数据（写作、事实问答），进行监督微调。

---

#### **三、性能优化**
1. **模型蒸馏**  
   - **数据与方法**：使用800K推理数据，通过纯SFT将DeepSeek-R1能力蒸馏至小型模型（Qwen/Llama系列）。  
   - **关键结论**：  
     - 蒸馏模型性能显著优于直接在小型模型上应用RL（如DeepSeek-R1-Distill-Qwen-32B在AIME 2024上pass@1达72.6%，对比RL训练的47%）。  
     - 蒸馏70B模型在GPQA Diamond（65.2% pass@1）和LiveCodeBench（57.5% pass@1）上超越OpenAI-o1-mini（见表5）。

---

#### **四、性能表现**
1. **教育领域**  
   - **MMLU**：90.8% pass@1，超越DeepSeek-V3（88.5%）和GPT-4o（87.2%）。  
   - **中文任务短板**：C-SimpleQA仅63.7%（低于DeepSeek-V3的68%），因安全对齐导致拒绝回答。  

2. **代码与数学**  
   - **Codeforces**：Elo评分2029，超越96.3%人类参赛者。  
   - **MATH-500**：97.3% pass@1，与OpenAI-o1-1217持平。  

3. **其他任务**  
   - **开放式生成**：AlpacaEval 2.0长度控制胜率87.6%，ArenaHard胜率92.3%。  
   - **长文本理解**：在FRAMES长文本QA任务中准确率达82.5%。

---

#### **五、自进化与关键现象**
1. **自演化过程**  
   - **推理能力涌现**：RL训练中自主发展出多步推理、自我验证和反思行为（如重新审视错误步骤）。  
   - **Aha Moment**：模型在中间版本出现“顿悟”行为（见表3），例如重新推导方程并修正错误，展示RL驱动的自主策略进化。  

2. **语言混合优化**  
   - 引入语言一致性奖励（目标语言词汇占比）导致推理性能轻微下降，但显著提升输出可读性。

---

#### **六、失败尝试分析**
1. **过程奖励模型（PRM）**  
   - **局限性**：难以定义细粒度推理步骤，自动化标注质量低，易引发奖励黑客问题。  

2. **蒙特卡洛树搜索（MCTS）**  
   - **挑战**：搜索空间指数爆炸，价值模型训练困难，难以复现AlphaGo式迭代提升。

---

#### **七、未来方向**
1. **通用能力增强**：探索长链推理（Long CoT）对函数调用、多轮对话等任务的提升。  
2. **语言混合解决**：优化多语言支持，减少非中/英文查询中的语言切换。  
3. **软件工程优化**：通过异步评估和拒绝采样加速RL在代码工程任务中的应用。  
4. **提示工程改进**：默认零样本设置，避免少样本提示导致的性能下降。

---

#### **八、关键对比**
| **模型/方法**     | **核心优势**                      | **局限性**                 |
| ----------------- | --------------------------------- | -------------------------- |
| DeepSeek-R1-Zero  | 纯RL（无SFT），自主演化推理能力   | 输出可读性差、语言混合     |
| DeepSeek-R1       | 冷启动+多阶段RL，性能对标SOTA     | 中文任务因安全对齐导致衰退 |
| 蒸馏模型（如32B） | SFT蒸馏经济高效，性能超越同规模RL | 依赖教师模型数据质量       |

---

**注**：公式与数据均严格对照论文原文修正，确保符号一致性和数值准确性。

