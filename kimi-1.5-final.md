### Kimi 1.5 技术报告

---

#### **1. 模型概述**  
- **无需复杂技术**：  
  Kimi 1.5 未依赖蒙特卡洛树搜索（MCTS）、价值函数或过程奖励模型等复杂技术，而是通过 **长上下文扩展** 和 **改进的策略优化方法** 实现高性能。  
- **多模态能力**：  
  支持文本与视觉数据的联合推理，包括：  
  - **科学图表解析**（如数学公式、几何图形）  
  - **OCR 文本识别**（多语言、密集排版、手写体）  
  - **图像对话**（基于真实场景的图像问答）  
  - **合成视觉推理**（空间关系、物体交互模拟）  

---

#### **2. 关键技术**  

##### **2.1 长上下文扩展**
Kimi 1.5 将上下文窗口扩展到 128k，显著提升了模型在复杂任务中的性能。
- **部分轨迹回放（Partial Rollouts）**：  
通过部分轨迹回放技术，模型可以高效地处理长上下文，避免了从头生成新轨迹的成本。
  - **分段处理**：将长轨迹分割为固定 token 预算的片段（如 128k），跨训练迭代逐步生成。  
  - **异步优化**：允许部分 Worker 处理长轨迹时，其他 Worker 并行处理短轨迹，最大化 GPU 利用率。  
  - **重复检测**：识别重复生成内容并提前终止，减少冗余计算。  

##### **2.2 改进的策略优化**  
- **在线镜像下降算法**：  
  目标函数为带 KL 散度正则化的奖励最大化（公式2）：  
  \[
  \max_{\theta} \mathbb{E}_{(x,y^*)\sim\mathcal{D}} \left[ \mathbb{E}_{(y,z)\sim\pi_{\theta}} \left[ r(x,y,y^*) \right] - \tau \text{KL}(\pi_{\theta}(x) || \pi_{\theta_i}(x)) \right]
  \]  
  梯度更新基于采样奖励与参考策略的差异（公式3）：  
  \[
  \frac{1}{k} \sum_{j=1}^k \nabla_{\theta} \log\pi_{\theta}(y_j,z_j|x) \cdot (r(x,y_j,y^*) - \overline{r}) 
  \]  
- **长度惩罚（Length Penalty）**：  
  - 对正确但冗长的回答施加负奖励（$\lambda = 0.5 - \frac{\text{len}(i) - \min\_\text{len}}{\max\_\text{len} - \min\_\text{len}}$），对错误长回答额外惩罚。  
  - 训练初期禁用长度惩罚，逐步引入以避免性能下降。  

---

#### **3. 训练方法**  

##### **3.1 RL 提示集策划**  
- **易被攻击提示过滤**：  
  - 若模型 **无推理步骤** 直接猜测答案，且在 $N=8$ 次尝试内成功，则剔除该提示（防止奖励作弊）。  
  - 排除多选题、判断题等易通过猜测得分的题型。      

##### **3.2 长链监督微调**  
- **长链推理的认知要素**：  
  - **规划**（系统化分解问题步骤）  
  - **评估**（对中间步骤的批判性检查）  
  - **反思**（回溯并修正错误路径）  
  - **探索**（尝试替代解决方案）  

---

#### **4. 实验结果**  

##### **4.1 长上下文扩展与模型规模**  
- **小模型补偿策略**：  
  - 通过扩展上下文长度（128k），小模型可接近大模型性能，但 **token 效率更低**（需生成更长推理链）。  
  - 大模型在相同上下文下表现更优且更高效（见图8）。  

##### **4.2 长链到短链方法**  
- **具体技术**：  
  - **模型合并**：长链与短链模型权重平均（无需训练）。  
  - **最短拒绝采样**：采样 $n=8$ 次，选择最短正确回答用于微调。  
  - **DPO**：以最短正确回答为正样本，长回答为负样本。  
  - **长链到短链 RL**：二次 RL 训练，限制最大生成长度并强化长度惩罚。  

---

#### **5. 其他补充**  
- **混合部署框架**：  
  - 训练（Megatron）与推理（vLLM）共享 GPU，通过 Kubernetes Sidecar 容器动态切换，减少资源闲置。  
- **代码沙盒**：  
  - 基于 Kubernetes 的安全代码执行环境，支持多编程语言测试用例生成与验证（通过 CYaRon 库）。  

---
### Kimi 1.5 与 DeepSeek-R1 的核心区别

| **维度**         | **Kimi 1.5**                                                 | **DeepSeek-R1**                                              |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **架构设计**     | 多模态Transformer解码器，支持128k长上下文，视觉与文本联合训练 | 纯文本/代码模型，基于DeepSeek-V3-Base架构，总参数量671B，激活参数37B |
| **训练框架**     | 基于在线镜像下降的RL，结合课程采样与优先级采样，无需价值函数 | 采用GRPO算法（组内评分替代Critic模型），分冷启动SFT+两阶段RL训练 |
| **多模态支持**   | 支持文本、图像（图表、OCR、视觉问答）的联合推理              | 未明确支持多模态，专注于文本与代码任务                       |
| **奖励设计**     | Chain-of-Thought奖励模型，评估推理过程与答案正确性           | 规则验证的准确性奖励+格式奖励（强制使用 `<think>` 和 `<answer>` 标签） |
| **长链推理优化** | 长上下文扩展（128k）+部分轨迹回放，支持隐式搜索空间          | 通过冷启动SFT和RL演化长链推理能力，未显式扩展上下文长度      |
| **性能优化手段** | 长链到短链方法（模型合并、最短拒绝采样、DPO）提升token效率   | 模型蒸馏（将RL能力迁移至小型模型，显著优于直接RL训练）       |
| **关键实验结果** | - AIME 2024（77.5）<br>- Codeforces（94百分位）<br>- MATH-500（96.2） | - Codeforces（Elo 2029，超越96.3%人类）<br>- GPQA Diamond（65.2%） |
| **失败尝试**     | 未提及                                                       | 过程奖励模型（PRM）和蒙特卡洛树搜索（MCTS）因标注质量与搜索空间问题被放弃 |
| **未来方向**     | 提升长上下文RL效率，改进信用分配，结合长链到短链迭代优化     | 优化多语言支持，探索长链推理对函数调用的提升，加速RL在代码工程中的应用 |

---

#### **核心差异总结**  

1. **技术路线**：  
   - Kimi 1.5 以**长上下文扩展**为核心，通过部分轨迹回放和简化的RL框架实现多模态推理；  
   - DeepSeek-R1 依赖**GRPO算法**和冷启动SFT，专注于文本/代码任务的策略进化与蒸馏。  

2. **多模态能力**：  
   - Kimi 1.5 强调视觉与文本的跨模态对齐（如OCR、图表理解）；  
   - DeepSeek-R1 未涉及多模态，聚焦于纯语言与代码任务的性能突破。  

3. **效率优化**：  
   - Kimi 1.5 通过长链到短链方法压缩推理长度；  
   - DeepSeek-R1 通过蒸馏技术实现小模型的高效部署。  

4. **应用场景**：  
   - Kimi 1.5 适用于需要长上下文和多模态推理的复杂任务（如学术问题解答）；  
   - DeepSeek-R1 更擅长代码生成与数学竞赛类高难度单模态任务。  
